{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdflib\n",
    "import rdflib\n",
    "from rdflib import Namespace , Literal , URIRef\n",
    "from rdflib.namespace import RDF , RDFS\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "# utils\n",
    "import ssl, os.path, json, requests , ast\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "# spacy\n",
    "import spacy\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "from pathlib import Path\n",
    "\n",
    "# python3 -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# data proc \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from apyori import apriori\n",
    "from efficient_apriori import apriori\n",
    "\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, fpmax, fpgrowth\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "# graph data\n",
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "\n",
    "# data viz\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# maps\n",
    "from ipywidgets import HTML\n",
    "from ipyleaflet import Map, Marker, Popup, LayersControl, AwesomeIcon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ1. What are the places of education and activity of the selected art historians?\n",
    "\n",
    "\n",
    "### Map of the places of art historians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center = (41.080684, -30.683374)\n",
    "m = Map(center=center, zoom=3, close_popup_on_click=False)\n",
    "\n",
    "if os.path.isfile(\"RQ1.json\"):\n",
    "    f = open('RQ1.json')\n",
    "    data_RQ1 = json.load(f)\n",
    "\n",
    "for place in data_RQ1:\n",
    "    historians = [list(item) for item in set(tuple(row) for row in place[\"historians\"])]\n",
    "    names = [ hist[1] for hist in historians ]\n",
    "    namelist = \"; \".join(names)\n",
    "    coords = ast.literal_eval(place[\"coords\"])\n",
    "    if place[\"type\"] == \"geoloc\":\n",
    "        icon2 = AwesomeIcon(\n",
    "        name = \"map-marker\",\n",
    "        marker_color='blue',\n",
    "        icon_color='white',\n",
    "        spin=False\n",
    "        )\n",
    "        marker = Marker(icon = icon2, location=(coords[1], coords[0]))\n",
    "        m.add_layer(marker)\n",
    "    else:\n",
    "        icon2 = AwesomeIcon(\n",
    "        name = \"bank\",\n",
    "        marker_color='green',\n",
    "        icon_color='white',\n",
    "        spin=False\n",
    "            )\n",
    "        marker = Marker(icon = icon2, location=(coords[1], coords[0]))\n",
    "        m.add_layer(marker)\n",
    "    \n",
    "    message = HTML()\n",
    "    marker.popup = message\n",
    "    message.description = \"\"\n",
    "    message.value = \"<b>\" + place[\"label\"] + \"</b>\" + \"<br/>\" + namelist\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map of the distribution of art historians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"RQ1.json\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['long','lat']] = df['coords'].str.strip('[]').str.split(', ', expand=True)\n",
    "#df['list'] = df['historians'].apply(lambda txt: ','.join(map(str, txt ) ) )\n",
    "df['people'] = [ \"; \".join([e[1] for e in i]) for i in df['historians'].tolist() ] \n",
    "df['count'] = df.historians.apply(len)\n",
    "df['long'] = df['long'].str.replace(\"'\",'').astype(float)\n",
    "df['lat'] = df['lat'].str.replace(\"'\",'').astype(float)\n",
    "tok = 'pk.eyJ1IjoibWFyaWxlbmFkYXF1aW5vIiwiYSI6ImNpeWx6MjZ3dTAwMjkzMmpyZGR2ejVqZHoifQ.AjYdPJPgptZiqrLSrrHSxQ'\n",
    "px.set_mapbox_access_token(tok)\n",
    "fig = px.scatter_mapbox(df, lat=\"lat\", lon=\"long\", size_max=40, color=\"type\",\n",
    "    size=\"count\", hover_data=[\"people\"], hover_name='label', zoom=2, title='Distribution of historians')\n",
    "fig.update_layout( mapbox_accesstoken=tok)\n",
    "#fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(df, x=\"label\", y=\"count\",  hover_data=[\"count\"], color=\"type\", hover_name='label', title='Distribution of historians')\n",
    "fig.update_layout( mapbox_accesstoken=tok)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most represented places are USA (New York and Los Angeles) and Europe: Rome, Munich and Freiburg, Florence and London.\n",
    "\n",
    "The most represented institutions include:\n",
    "\n",
    " * USA\n",
    "   * East Coast: NY Universities and museums. Harvard University\n",
    "   * West Coast: Los Angeles, The Getty Museum\n",
    " * Europe\n",
    "   * Italy: Rome area and Sapienza University. Florence and Villa I Tatti. Bologna. Turin and the University of Turin.\n",
    "   * Germany: Munich and the Max Planck Society. Berlin and the Humboldt University. Friedburg\n",
    "   * UK: London, the British Museum and the National Gallery. Oxford and its colleges. Birmingham "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relations between art historians \n",
    "\n",
    "Based on the network of places and institutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = df['people'].tolist()\n",
    "people = [ list(set([el]))  if ';' not in el else list(set([i for i in el.split('; ')])) for el in people]\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(people).transform(people)\n",
    "dfrel = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "frequent_itemsets_historians = fpgrowth(dfrel, min_support=0.02, use_colnames=True)\n",
    "rules = association_rules(frequent_itemsets_historians, metric=\"confidence\", min_threshold=0.1)\n",
    "rules['lhs items'] = rules['antecedents'].apply(lambda x:len(x) )\n",
    "rules[rules['lhs items']>1].sort_values('lift', ascending=False).head()\n",
    "# Replace frozen sets with strings\n",
    "rules['antecedents_'] = rules['antecedents'].apply(lambda a: ','.join(list(a)))\n",
    "rules['consequents_'] = rules['consequents'].apply(lambda a: ','.join(list(a)))\n",
    "# Transform the DataFrame of rules into a matrix using the lift metric\n",
    "pivot = rules[rules['lhs items']>1].pivot(index = 'antecedents_', columns = 'consequents_', values= 'lift')\n",
    "# Generate a heatmap with annotations on and the colorbar off\n",
    "sns.heatmap(pivot, annot = True)\n",
    "plt.yticks(rotation=0)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Significant cooccurrence of historians:\n",
    " \n",
    " * W. Cohn, R. Krautimer, L. Steinberg\n",
    " * E.Steinemann, R. Krautimer, W. Lotz\n",
    " * O.Brockhaus, R. Krautimer, W. Lotz\n",
    " * F. Zeri, L. Steinberg, L. Salerno\n",
    " * W. Cohn, K. Badt\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on only institutions they have in common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfinst = df.copy()\n",
    "dfinst = dfinst.drop(dfinst[dfinst[\"type\"] == 'geoloc'].index)\n",
    "dfinst.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_inst = dfinst['people'].tolist()\n",
    "people_inst = [ list(set([el]))  if ';' not in el else list(set([i for i in el.split('; ')])) for el in people_inst]\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(people_inst).transform(people_inst)\n",
    "dfinstt = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "frequent_itemsets_historians_inst = fpgrowth(dfinstt, min_support=0.02, use_colnames=True)\n",
    "rules = association_rules(frequent_itemsets_historians_inst, metric=\"confidence\", min_threshold=0.1)\n",
    "rules['lhs items'] = rules['antecedents'].apply(lambda x:len(x) )\n",
    "rules[rules['lhs items']>1].sort_values('lift', ascending=False).head()\n",
    "# Replace frozen sets with strings\n",
    "rules['antecedents_'] = rules['antecedents'].apply(lambda a: ','.join(list(a)))\n",
    "rules['consequents_'] = rules['consequents'].apply(lambda a: ','.join(list(a)))\n",
    "# Transform the DataFrame of rules into a matrix using the lift metric\n",
    "pivot = rules[rules['lhs items']>1].pivot(index = 'antecedents_', columns = 'consequents_', values= 'lift')\n",
    "# Generate a heatmap with annotations on and the colorbar off\n",
    "sns.heatmap(pivot, annot = True)\n",
    "plt.yticks(rotation=0)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Significant cooccurrences:\n",
    " \n",
    " * E. Steinmann , O. L-Brockhaus\n",
    " * E. Fahy + L. Steinberg, F. Zeri\n",
    " * W. Cohn, W. Lotz\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network of historians \n",
    "\n",
    "Based on places and institutions in common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_names = set(itertools.chain.from_iterable(people))\n",
    "\n",
    "# Get all combinations of pairs\n",
    "all_pairs = list(itertools.combinations(unique_names, 2))\n",
    "\n",
    "# Create the dictionary\n",
    "hp_relations = [ (pair[0], pair[1], len([x for x in people if set(pair) <= set(x)]) ) for pair in all_pairs]\n",
    "hp_relations = [ pair for pair in hp_relations if pair[2] >0]\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_weighted_edges_from(hp_relations)\n",
    "\n",
    "plt.figure(1,figsize=(100,100)) \n",
    "\n",
    "pos=nx.circular_layout(G) # pos = nx.nx_agraph.graphviz_layout(G)\n",
    "nx.draw_networkx_nodes(G,pos,node_color='green',node_size=7500)\n",
    " \n",
    "#3. If you want, add labels to the nodes\n",
    "labels = {}\n",
    "for node_name in unique_names:\n",
    "    labels[str(node_name)] =str(node_name)\n",
    "\n",
    "nx.draw_networkx_labels(G,pos,labels,font_size=120)\n",
    "\n",
    "all_weights = []\n",
    "#4 a. Iterate through the graph nodes to gather all the weights\n",
    "for (node1,node2,data) in G.edges(data=True):\n",
    "    all_weights.append(data['weight']) #we'll use this when determining edge thickness\n",
    "\n",
    "#4 b. Get unique weights\n",
    "unique_weights = list(set(all_weights))\n",
    "\n",
    "#4 c. Plot the edges - one by one!\n",
    "for weight in unique_weights:\n",
    "    weighted_edges = [(node1,node2) for (node1,node2,edge_attr) in G.edges(data=True) if edge_attr['weight']==weight]\n",
    "    nx.draw_networkx_edges(G,pos,edgelist=weighted_edges,edge_color='g', width=weight*7.0, alpha= 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most connected historians seem to be:\n",
    "\n",
    " * R. Krautheimer > W. Lotz (confirmed in the biography of Lotz and collection of R.K), L. Steinberg and W. Cohn (missing E. Steinmann)\n",
    " * W. Lotz > R. Krautheimer, Aby Warburg and W. Cohn\n",
    " * W. Cohn > R. Krautheimer, L. Steinberg, U. Middeldorf, W. Lotz and E. Steinmann\n",
    " * F. Zeri > L. Steinberg, L. Salerno, E. Waterhouse (missing J. Pope-Hennessy and R. Longhi)\n",
    " * L. Salerno > F. Zeri and L. Steinberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historians' degree of centrality in the network of academic relations\n",
    "\n",
    "**<font color='red'>Influential historians</font>** What are the most influential historians? Based on the network of places they are connected to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cent = nx.degree_centrality(G)\n",
    "sorted_centrality = {k: v for k, v in sorted(cent.items(), key=lambda item: item[1])}\n",
    "plt.bar(range(len(sorted_centrality)), list(sorted_centrality.values()), align='center')\n",
    "plt.xticks(range(len(sorted_centrality)), list(sorted_centrality.keys()), rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Central historians (with a degree higher than 0.8) are:\n",
    "\n",
    " * F. Zeri\n",
    " * L. Steinberg\n",
    " * W. Lotz\n",
    " * L. Salerno\n",
    " * L. Vertova"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relations between places and institutions \n",
    "\n",
    "Based on historians' paths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfh = pd.read_json(\"RQ1_hist.json\")\n",
    "places = dfh['places'].tolist()\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(places).transform(places)\n",
    "dfp = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "frequent_itemsets = fpgrowth(dfp, min_support=0.1, use_colnames=True)\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)\n",
    "rules['lhs items'] = rules['antecedents'].apply(lambda x:len(x) )\n",
    "rules[rules['lhs items']>=1].sort_values('lift', ascending=False).head()\n",
    "# Replace frozen sets with strings\n",
    "rules['antecedents_'] = rules['antecedents'].apply(lambda a: ','.join(list(a)))\n",
    "rules['consequents_'] = rules['consequents'].apply(lambda a: ','.join(list(a)))\n",
    "# Transform the DataFrame of rules into a matrix using the lift metric\n",
    "pivot = rules[rules['lhs items']>=1].pivot(index = 'antecedents_', \n",
    "                    columns = 'consequents_', values= 'lift')\n",
    "# Generate a heatmap with annotations on and the colorbar off\n",
    "sns.heatmap(pivot, annot = True)\n",
    "plt.yticks(rotation=0)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most relevant rules:\n",
    " * Rule 0-3: Hertziana, Rome and Germany / Munich\n",
    " * Rule 7-8: New York University and Berlin. \n",
    " * Rule 5-6: London or USA and Rome. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relations between institutions \n",
    "\n",
    "Based on historians' paths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "institutions = [[place for place in place_list if \"(loc)\" not in place] for place_list in places ]\n",
    "institutions = [x for x in institutions if x]\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(places).transform(institutions)\n",
    "dfi = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "frequent_itemsets = fpgrowth(dfi, min_support=0.05, use_colnames=True)\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)\n",
    "rules['lhs items'] = rules['antecedents'].apply(lambda x:len(x) )\n",
    "rules[rules['lhs items']>=1].sort_values('lift', ascending=False).head()\n",
    "# Replace frozen sets with strings\n",
    "rules['antecedents_'] = rules['antecedents'].apply(lambda a: ','.join(list(a)))\n",
    "rules['consequents_'] = rules['consequents'].apply(lambda a: ','.join(list(a)))\n",
    "# Transform the DataFrame of rules into a matrix using the lift metric\n",
    "pivot = rules[rules['lhs items']>=1].pivot(index = 'antecedents_', \n",
    "                    columns = 'consequents_', values= 'lift')\n",
    "# Generate a heatmap with annotations on and the colorbar off\n",
    "sns.heatmap(pivot, annot = True)\n",
    "plt.yticks(rotation=0)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Significant cooccurrence of institutions:\n",
    "\n",
    " * Hertziana, NY University, and Vassar College\n",
    " * Columbia University, NY University, and MET museum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "institutions = [tuple(l) for l in institutions]\n",
    "itemsets, rules = apriori(institutions, min_support=0.05, min_confidence=0.7, output_transaction_ids=True)\n",
    "print(itemsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
